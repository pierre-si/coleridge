{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0c875a887c8abb6fe16ed242bf424b28894bcb5ee769b5b7b2d55a08b56eb3e6d",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Transformer on the full-sentences extracts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_full-sentences_cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         Id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "4      000efc17-13d8-433d-8f62-a3932fe4f3b8   \n",
       "...                                     ...   \n",
       "51752  ffd4d86a-0f26-44cc-baed-f0e209cc22af   \n",
       "51753  ffe7f334-245a-4de7-b600-d7ff4e28bfca   \n",
       "51754  ffeb3568-7aed-4dbe-b177-cbd7f46f34af   \n",
       "51755  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "51756  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "\n",
       "                                          section_title  \\\n",
       "0                                          Introduction   \n",
       "1                                     LITERATURE REVIEW   \n",
       "2        Example: Farm Income and Farm Household Wealth   \n",
       "3                                            Highlights   \n",
       "4                                        Study subjects   \n",
       "...                                                 ...   \n",
       "51752                    II.1. MRI Brain Image database   \n",
       "51753          Characterization of the SARS-CoV-2 virus   \n",
       "51754  Polish research on the perception of mathematics   \n",
       "51755                                               NaN   \n",
       "51756                                               NaN   \n",
       "\n",
       "                                                sentence  \\\n",
       "0      in fact organizations are now identifying digi...   \n",
       "1      international studies on student achievement s...   \n",
       "2      the agricultural resources management survey a...   \n",
       "3      1 manages access to results of the agricultura...   \n",
       "4      the adni data set is from a multicenter longit...   \n",
       "...                                                  ...   \n",
       "51752  data used in the preparation of this article w...   \n",
       "51753  interestingly the genome sequences of sars cov...   \n",
       "51754  as part of the program for international stude...   \n",
       "51755  analysis considered first time beginning posts...   \n",
       "51756  my prior research illustrated with use of begi...   \n",
       "\n",
       "                                           dataset_label  label_length  \n",
       "0      program for the international assessment of ad...            62  \n",
       "1      trends in international mathematics and scienc...            53  \n",
       "2               agricultural resources management survey            40  \n",
       "3               agricultural resources management survey            40  \n",
       "4                                                   adni             4  \n",
       "...                                                  ...           ...  \n",
       "51752  alzheimer s disease neuroimaging initiative adni             49  \n",
       "51753                     genome sequences of sars cov 2            30  \n",
       "51754  trends in international mathematics and scienc...            53  \n",
       "51755                   beginning postsecondary students            32  \n",
       "51756                    beginning postsecondary student            31  \n",
       "\n",
       "[51757 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>section_title</th>\n      <th>sentence</th>\n      <th>dataset_label</th>\n      <th>label_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n      <td>Introduction</td>\n      <td>in fact organizations are now identifying digi...</td>\n      <td>program for the international assessment of ad...</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n      <td>LITERATURE REVIEW</td>\n      <td>international studies on student achievement s...</td>\n      <td>trends in international mathematics and scienc...</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n      <td>Example: Farm Income and Farm Household Wealth</td>\n      <td>the agricultural resources management survey a...</td>\n      <td>agricultural resources management survey</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n      <td>Highlights</td>\n      <td>1 manages access to results of the agricultura...</td>\n      <td>agricultural resources management survey</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n      <td>Study subjects</td>\n      <td>the adni data set is from a multicenter longit...</td>\n      <td>adni</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51752</th>\n      <td>ffd4d86a-0f26-44cc-baed-f0e209cc22af</td>\n      <td>II.1. MRI Brain Image database</td>\n      <td>data used in the preparation of this article w...</td>\n      <td>alzheimer s disease neuroimaging initiative adni</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>51753</th>\n      <td>ffe7f334-245a-4de7-b600-d7ff4e28bfca</td>\n      <td>Characterization of the SARS-CoV-2 virus</td>\n      <td>interestingly the genome sequences of sars cov...</td>\n      <td>genome sequences of sars cov 2</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>51754</th>\n      <td>ffeb3568-7aed-4dbe-b177-cbd7f46f34af</td>\n      <td>Polish research on the perception of mathematics</td>\n      <td>as part of the program for international stude...</td>\n      <td>trends in international mathematics and scienc...</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>51755</th>\n      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n      <td>NaN</td>\n      <td>analysis considered first time beginning posts...</td>\n      <td>beginning postsecondary students</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>51756</th>\n      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n      <td>NaN</td>\n      <td>my prior research illustrated with use of begi...</td>\n      <td>beginning postsecondary student</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n<p>51757 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "# Generate tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(sentence, label):\n",
    "    tokens_sequence = ['O'] * len(sentence.split())\n",
    "    start_char = sentence.find(label)\n",
    "    start_token = len(sentence[:start_char].split())\n",
    "\n",
    "    label_len = len(label.split())\n",
    "    tokens_sequence[start_token:start_token+label_len] = ['D']*label_len\n",
    "    return tokens_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "tags = []\n",
    "for i, row in df.iterrows():\n",
    "    texts.append(row.sentence.split())\n",
    "    tags.append(generate_tokens(row.sentence, row.dataset_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_texts = [text for text in texts if len(text) <= 128]\n",
    "short_tags = [tag for text, tag in zip(texts, tags) if len(text) <= 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_texts_idx = [i for i in range(len(texts)) if len(texts[i]) > 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = short_texts\n",
    "tags = short_tags"
   ]
  },
  {
   "source": [
    "# Writing data to file\n",
    "To be used with the run_ner script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(filename, texts, tags):\n",
    "    with open(filename, 'w') as f:\n",
    "        for text, tag in zip(texts, tags):\n",
    "            json_el = {'tokens': text, 'tags': tag}\n",
    "            json.dump(json_el, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)\n",
    "print(len(train_texts), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json('cleaned_ner_train.json', train_texts, train_tags)\n",
    "save_json('cleaned_ner_val.json', val_texts, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json('cleaned_ner_train_small.json', train_texts[:4000], train_tags[:4000])\n",
    "save_json('cleaned_ner_val.json', val_texts[:2000], val_tags[:2000])"
   ]
  },
  {
   "source": [
    "# Training in the notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "max_length = 512\n",
    "train_encodings = tokenizer(train_texts, max_length=max_length, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, max_length=max_length, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "NumPy boolean array indexing assignment cannot assign 12 input values to the 11 output values where the mask is true",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-465a3f04008d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-465a3f04008d>\u001b[0m in \u001b[0;36mencode_tags\u001b[0;34m(tags, encodings)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(doc_enc_labels.shape, len(doc_labels))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# set labels whose first offset position is 0 and the second is not 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdoc_enc_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_offset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr_offset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mencoded_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_enc_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: NumPy boolean array indexing assignment cannot assign 12 input values to the 11 output values where the mask is true"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        #print(doc_enc_labels.shape, len(doc_labels))\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[text_column_name],\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs"
   ]
  }
 ]
}