{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd02820280be186ab661f9de35bd01829c341881105846ede977025339161cc4480",
   "display_name": "Python 3.9.2 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "04/11/2021 11:35:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/11/2021 11:35:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr11_11-35-44_debian, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
      "04/11/2021 11:35:44 - WARNING - datasets.builder -   Using custom data configuration default-6903a2cf09be8547\n",
      "04/11/2021 11:35:44 - WARNING - datasets.builder -   Reusing dataset json (/home/pierre/.cache/huggingface/datasets/json/default-6903a2cf09be8547/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
      "[INFO|configuration_utils.py:490] 2021-04-11 11:35:46,617 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/pierre/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.1eb486b9eb195cc830038c19b140bd322f72283bc9f74df3151387785613845e\n",
      "[INFO|configuration_utils.py:526] 2021-04-11 11:35:46,618 >> Model config BigBirdConfig {\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_fast\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:490] 2021-04-11 11:35:47,024 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /home/pierre/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.1eb486b9eb195cc830038c19b140bd322f72283bc9f74df3151387785613845e\n",
      "[INFO|configuration_utils.py:526] 2021-04-11 11:35:47,025 >> Model config BigBirdConfig {\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_fast\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-04-11 11:35:49,257 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/spiece.model from cache at /home/pierre/.cache/huggingface/transformers/d318d7bb69cafb1d8964fc87515592ac3092a2c8fdb305068f9ba4020df3ee3b.271d467a9adc15fb44348481bc75c48b63cba0fd4934bc5377d63a63de052c45\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-04-11 11:35:49,258 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-04-11 11:35:49,258 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/special_tokens_map.json from cache at /home/pierre/.cache/huggingface/transformers/400be7e354ea6eb77319bcc7fa34899ec9fa2e3aff0fa677f6eb7e45a01b1548.75b358ecb30fa6b001d9d87bfde336c02d9123e7a8f5b90cc890d0f6efc3d4a3\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-04-11 11:35:49,258 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer_config.json from cache at /home/pierre/.cache/huggingface/transformers/d20a688e918d227ce5dbcd5f2b570a093cee6b095952d74b9c245b245e6510de.c8f14f85d9ff88cdd1fe7094cde11f85b74fcb7eb03616822964895bc6626c3b\n",
      "[INFO|tokenization_utils_base.py:1707] 2021-04-11 11:35:49,258 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer.json from cache at None\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "[INFO|modeling_utils.py:1052] 2021-04-11 11:35:49,775 >> loading weights file https://huggingface.co/google/bigbird-roberta-base/resolve/main/pytorch_model.bin from cache at /home/pierre/.cache/huggingface/transformers/c523b12608662dbff39b2c24a608a6ff30857bc7967a5c9b00cb76d1147e223b.06e7996caf35449212f17d31a2129bb55c59c19054fcf8552a847e4bcb475688\n",
      "[WARNING|modeling_utils.py:1159] 2021-04-11 11:35:56,315 >> Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1170] 2021-04-11 11:35:56,316 >> Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pierre/git/pierre-si/coleridge/bigbird-finetuning/../usr/lib/runner/runner.py\", line 404, in <module>\n",
      "    main()\n",
      "  File \"/home/pierre/git/pierre-si/coleridge/bigbird-finetuning/../usr/lib/runner/runner.py\", line 260, in main\n",
      "    raise ValueError(\n",
      "ValueError: This example script only works for models that have a fast tokenizer. Checkout the big table of models at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this requirement\n"
     ]
    }
   ],
   "source": [
    "!python ../usr/lib/runner/runner.py \\\n",
    "    --model_name_or_path \"google/bigbird-roberta-base\" \\\n",
    "    --train_file ../input/coleridge-sentences/ner_train-256.json \\\n",
    "    --validation_file ../input/coleridge-sentences/ner_val-256.json \\\n",
    "    --save_steps 500 \\\n",
    "    --output_dir output/ \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --do_train --do_eval --num_train_epochs 3"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdTokenizer, BigBirdForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.BigBirdConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 846k/846k [00:01<00:00, 608kB/s]\n",
      "Downloading: 100%|██████████| 775/775 [00:00<00:00, 314kB/s]\n",
      "Downloading: 100%|██████████| 1.02k/1.02k [00:00<00:00, 353kB/s]\n",
      "Downloading: 100%|██████████| 513M/513M [16:25<00:00, 520kB/s]\n",
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "model = BigBirdForTokenClassification.from_pretrained('google/bigbird-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
