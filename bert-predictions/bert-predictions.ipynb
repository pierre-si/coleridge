{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd02820280be186ab661f9de35bd01829c341881105846ede977025339161cc4480",
   "display_name": "Python 3.9.2 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "loc = os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost')\n",
    "if loc == 'Batch' or loc == 'Interactive':\n",
    "    !pip install --no-index --find-links=../input/coleridge-packages/datasets datasets\n",
    "    !pip install --no-index --find-links=../input/coleridge-packages/seqeval seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add utility_scripts in the current path so that they can be imported directly just like in interactive mode\n",
    "sys.path.append(os.path.abspath(\"../usr/lib/\"))\n",
    "for script_folder in os.listdir(\"../usr/lib/\"):\n",
    "    sys.path.append(os.path.abspath(\"../usr/lib/\"+script_folder))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from coleridgeutils import publications_cleaned_sentences_json, predictions_to_submission"
   ]
  },
  {
   "source": [
    "## Convert publications test folder to jsonl sentences file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_path = Path('../input/coleridgeinitiative-show-us-the-data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_cleaned_sentences_json(publications_path)"
   ]
  },
  {
   "source": [
    "## Make predictions using run_ner"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "04/09/2021 15:09:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/09/2021 15:09:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr09_15-09-33_debian, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
      "04/09/2021 15:09:34 - WARNING - datasets.builder -   Using custom data configuration default-e7b23a17d63f5932\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/pierre/.cache/huggingface/datasets/json/default-e7b23a17d63f5932/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n",
      "Dataset json downloaded and prepared to /home/pierre/.cache/huggingface/datasets/json/default-e7b23a17d63f5932/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:461] 2021-04-09 15:09:34,515 >> loading configuration file ../input/bertfinetuned/config.json\n",
      "[INFO|configuration_utils.py:499] 2021-04-09 15:09:34,516 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:461] 2021-04-09 15:09:34,516 >> loading configuration file ../input/bertfinetuned/config.json\n",
      "[INFO|configuration_utils.py:499] 2021-04-09 15:09:34,517 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-04-09 15:09:34,517 >> Didn't find file ../input/bertfinetuned/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-04-09 15:09:34,517 >> Didn't find file ../input/bertfinetuned/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-04-09 15:09:34,517 >> loading file ../input/bertfinetuned/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-04-09 15:09:34,517 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-04-09 15:09:34,518 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-04-09 15:09:34,518 >> loading file ../input/bertfinetuned/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-04-09 15:09:34,518 >> loading file ../input/bertfinetuned/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:1049] 2021-04-09 15:09:34,562 >> loading weights file ../input/bertfinetuned/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1167] 2021-04-09 15:09:37,518 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1175] 2021-04-09 15:09:37,518 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at ../input/bertfinetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  5.56ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.00ba/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  8.97ba/s]\n",
      "04/09/2021 15:09:40 - INFO - __main__ -   *** Predict ***\n",
      "[INFO|trainer.py:482] 2021-04-09 15:09:40,775 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: section_title, sentence, Id, tokens, ner_tags.\n",
      "[INFO|trainer.py:1775] 2021-04-09 15:09:40,777 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:1776] 2021-04-09 15:09:40,778 >>   Num examples = 2035\n",
      "[INFO|trainer.py:1777] 2021-04-09 15:09:40,778 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 255/255 [00:17<00:00, 18.58it/s]/home/pierre/programmes/anaconda3/envs/torch/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/pierre/programmes/anaconda3/envs/torch/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pierre/programmes/anaconda3/envs/torch/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pierre/programmes/anaconda3/envs/torch/lib/python3.9/site-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_loss = 0.043842244893312454\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_accuracy_score = 0.9916790547715119\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_precision = 0.0\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_recall = 0.0\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_f1 = 0.0\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_runtime = 19.1551\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     eval_samples_per_second = 106.238\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     init_mem_cpu_alloc_delta = 2415660\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     init_mem_gpu_alloc_delta = 436709888\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     init_mem_cpu_peaked_delta = 18258\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     init_mem_gpu_peaked_delta = 0\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     test_mem_cpu_alloc_delta = 4484128\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     test_mem_gpu_alloc_delta = 0\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     test_mem_cpu_peaked_delta = 4295730\n",
      "04/09/2021 15:10:00 - INFO - __main__ -     test_mem_gpu_peaked_delta = 35496960\n",
      "100%|█████████████████████████████████████████| 255/255 [00:18<00:00, 13.47it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../usr/lib/runner/runner.py --model_name_or_path ../input/bertfinetuned --train_file ../input/bertfinetuned/ner_train-128.json --validation_file ../input/bertfinetuned/ner_val-128.json --test_file sentences.json --output_dir output/ --do_predict"
   ]
  },
  {
   "source": [
    "## Convert predictions to submission file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_submission(Path('sentences.json'), Path('output/test_predictions.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}